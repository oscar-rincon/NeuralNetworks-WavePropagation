{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation of the 1-D Wave Equation With PINNs Using PyTorch\n",
    "\n",
    "This notebook presents an example of approximating the solution to the 1-D wave equation using phyisics informed neural networks. The equation approximated in this notebook is the one-dimensional wave equation, which is a fundamental equation in physics that describes wave propagation. The equation is given by:\n",
    "$$\n",
    "\\begin{aligned} \\partial_t^2 u(x, t)=c^2 \\partial_x^2 u(x, t) & \\text { for all } 0<x<1 \\text { and } t>0 \\\\ u(0, t)=u(1, t)=0 & \\text { for all } t>0, \\\\ u(x, 0)=x(1-x) & \\text { for all } 0<x<1 \\\\ \\partial_t u(x, 0)=0 & \\text { for all } 0<x<1\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this equation, $u(x, t)$ represents the wave function, which gives the position of the wave at location $x$ and time $t$. The term $c^2$ is the square of the wave speed, and $∂_t^2$ and $∂_x^2$ are the second derivatives with respect to time and space, respectively. \n",
    "\n",
    "\n",
    "In the following code block, we import the necessary libraries for our notebook. This includes NumPy for numerical operations, PyTorch for building and training the neural network, Matplotlib for plotting, a utility module for additional plotting functions, and the time module for timing our training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Import PyTorch for building and training neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import Matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import a utility module for additional plotting functions\n",
    "import utils_plots\n",
    "\n",
    "# Import the time module to time our training process\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, we define a function for the analytical solution of the 1D wave equation. We then generate training data in NumPy, create a grid of `x` and `t` values, and calculate the corresponding u values. Finally, we plot the u values as a function of `x` and `t`.\n",
    "\n",
    "The analytical solution to the wave equation is given by the function:\n",
    "\n",
    "$$\n",
    "u(t, x) = \\sum_{k=1,...,n} \\frac{8}{k^3 \\pi^3} \\sin(k \\pi x) \\cos(C k \\pi t)\n",
    "$$\n",
    "\n",
    "In this equation, `C` is a constant set to 1, `k` is an integer that ranges from 1 to `n`, `x` represents the spatial position, and `t` represents time. The sine and cosine functions create a wave pattern in the `x` and `t` dimensions, respectively. The fraction in front of the sine and cosine functions ensures that the wave pattern diminishes for larger values of `k`.\n",
    "\n",
    "This function is used to generate the `u` values, which represent the wave's displacement at each point in our grid of `x` (spatial position) and `t` (time) values. This grid forms the basis for our training data, which we will use to train our neural network to approximate the wave equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the analytical solution of the 1D wave equation\n",
    "def analytic_sol_func(t, x):\n",
    "    C = 1\n",
    "    return sum([(8 / (k**3 * np.pi**3)) * np.sin(k * np.pi * x) * np.cos(C * k * np.pi * t) for k in range(1, 100, 2)])\n",
    "\n",
    "# Generate training data in NumPy\n",
    "x_np = np.linspace(0, 1, 100)  # x data (numpy array), shape=(100,)\n",
    "t_np = np.linspace(0, 1, 100)  # t data (numpy array), shape=(100,)\n",
    "\n",
    "# Create a grid of x and t values\n",
    "x_grid, t_grid = np.meshgrid(x_np, t_np) # x and t data (numpy array), shape=(100, 100)\n",
    "\n",
    "# Calculate u values using the analytical solution function\n",
    "u_grid = analytic_sol_func(t_grid,x_grid) # u data (numpy array), shape=(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, we convert our grid data from NumPy arrays to PyTorch tensors, which can be processed more efficiently by our neural network. We also concatenate the spatial (`x`) and temporal (`t`) tensors to form our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of the grid data to PyTorch tensors\n",
    "x = torch.from_numpy(x_grid).float().unsqueeze(-1).requires_grad_(True)\n",
    "t = torch.from_numpy(t_grid).float().unsqueeze(-1).requires_grad_(True)\n",
    "u = torch.from_numpy(u_grid).float().unsqueeze(-1)\n",
    "\n",
    "# Concatenation of x and y to form the input data\n",
    "input_data = torch.cat((x, t), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, we define our neural network model. This model is a simple feed-forward neural network with **two** hidden layers, each containing **50 neurons**. The activation function used is the hyperbolic tangent (`tanh`). We also instantiate the model, and define the optimizer (**Adam**) and the loss function (**Mean Squared Error**) to be used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network class with three fully connected layers\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, 50)\n",
    "        self.layer2 = nn.Linear(50, 50)\n",
    "        self.output_layer = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        x = torch.tanh(self.layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "# Create an instance of the neural network\n",
    "neural_net = NeuralNetwork()\n",
    "\n",
    "# Define an optimizer (Adam) for training the network\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.01)\n",
    "\n",
    "# Define a loss function (Mean Squared Error) for training the network\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, we train our neural network. We run the training process for 500 iterations, during which we feed the input data to the network, \n",
    "\n",
    "$$\n",
    "\\partial_t^2 u(x, t) - c^2 \\partial_x^2 u(x, t) = 0 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss 0.00294599705375731\n",
      "Iteration 100: Loss 0.00017846233095042408\n",
      "Iteration 200: Loss 8.040277316467836e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m      9\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m neural_net(input_data)     \u001b[38;5;66;03m# input x and predict based on x\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     dudx  \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m# computes du/dx\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     dudx2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(dudx,  x, torch\u001b[38;5;241m.\u001b[39mones_like(dudx),  create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m# computes d^2u/dx^2\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     dudt  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(prediction, t, torch\u001b[38;5;241m.\u001b[39mones_like(prediction), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;66;03m# computes du/dt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pinn/lib/python3.12/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the loss values\n",
    "loss_values = []\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the neural network\n",
    "for i in range(1000):\n",
    "    prediction = neural_net(input_data)     # input x and predict based on x\n",
    "    dudx  = torch.autograd.grad(prediction, x, torch.ones_like(prediction), create_graph=True)[0] # computes du/dx\n",
    "    dudx2 = torch.autograd.grad(dudx,  x, torch.ones_like(dudx),  create_graph=True)[0] # computes d^2u/dx^2\n",
    "    dudt  = torch.autograd.grad(prediction, t, torch.ones_like(prediction), create_graph=True)[0] # computes du/dt\n",
    "    dudt2 = torch.autograd.grad(dudt,  t, torch.ones_like(dudt),  create_graph=True)[0] # computes d^2u/dt^2\n",
    "    loss = dudt2 - dudx2 #loss_func(prediction, u)     # must be (1. nn output, 2. target)\n",
    "    \n",
    "    # Append the current loss value to the list\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if i % 100 == 0:  # print every 100 iterations\n",
    "        print(f\"Iteration {i}: Loss {loss.item()}\")\n",
    "    \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "\n",
    "# Stop the timer and calculate the elapsed time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training time: {elapsed_time} seconds\")\n",
    "\n",
    "# Save a summary of the training process to a text file\n",
    "with open(\"summaries/summary_1D_Wave_Equation_NN_Model.txt\", \"w\") as file:\n",
    "    file.write(f\"Training time: {elapsed_time} seconds\\n\")\n",
    "    file.write(f\"Number of iterations: {len(loss_values)}\\n\")\n",
    "    file.write(f\"Initial loss: {loss_values[0]}\\n\")\n",
    "    file.write(f\"Final loss: {loss_values[-1]}\\n\")\n",
    "    file.write(f\"Neural network architecture: {neural_net}\\n\")\n",
    "    file.write(f\"Optimizer used: {type(optimizer).__name__}\\n\")\n",
    "    file.write(f\"Learning rate: {optimizer.param_groups[0]['lr']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
